# ClaveNet-Evaluation

This repo houses three important ClaveNet modules for evaluation:
1. Perform a rhythmic-feature based comparison scheme between an evaluation set and a generated set. Note that there is a 1:1 relationship between generated examples and evaluation examples.
2. An interface for evaluating a single model based on the comparison scheme.
3. Based on the comparison scheme results, compute a cumulative distance (CD) as an overall score for a model, and then rank multiple models by their CDs.

## Comparison module

The module in `evaluation/comparison.py` runs a comparison scheme that essentially estimates probability density functions (pdfs) from the feature distances between an evaluation set and a generated set and, based on these pdfs, then computes two distance metrics: KL-Divergence (KL-D) and Overlapping area (OA). This module is called by the evaluation module. 

Refer to `evaluation/constants.py` for the list of rhythmic features.

### Arguments

* **Generated set**: Set of examples generated by a model, prompted by a monotonic groove.
* **Evaluation set**: Our "target" set of drum examples
* **Features to extract**: Rhythmic features used to compare the two sets
* **Simple (Return KDE or just mean and std)**: If `simple=False`, the KDEs will be pickled as part of the output. Otherwise, the mean and std of these will be provided as part of the output csv. 
* **Number of points**: Number of points used to estimate pdf with KDE and to calculate distance metrics.
* **Padding factor**: Factor used to pad the range of the KDEs

### Output

A `ComparisonResult` object, which include the two distance metrics between the two sets, KDEs for the *intra*set distances of both the generated set and evaluation set as well as a KDE for *inter*set distances, and the number of points.

## Evaluation module

The module in `evaluation/evaluation.py` processes the evaluation set, creates a monotonic set based on the evaluation set, uses the montonic set and the provided model to create a generated set, and then runs the comparison module between the evaluation set and the generated set.

### Arguments

* **Model to evaluate**: Provide the path to the model that will be evaluated via comparison.
* **Evaluation Set**: Provide the path to the evaluation set that will be compared to the model's generated set.
* **Number of points**: Same argument as comparison module. Default is 10000.
* **Simple**: Same argument as comparison module.
* **Output dir**: Provide the path to where the results will be written.

### Output

A directory named as follows:

```
<model-name>_evaluation_<evaluation-time>
|-- results.csv
|-- results.pkl

```
* **results.csv** For each rhythmic feature, shows KL-D and OA as well mean and standard deviation for all KDE-estimated pdfs.
* **results.pkl** (Optional) Pickled KDEs.

### Evaluation batch

The script `eval_batch.py` can be used to run the evaluation module on multiple models sequentially.  

#### Usage

1. Move models to be evaluated to the `models/` dir.
2. Move evaluation set to the root dir. Change the `EVALUAION_DATASET` constant to the name of the evaluation dataset.
3. ```$ python eval_batch.py```

Output will be written to `eval_out/`.

## Analysis module

The module in `evaluation/analysis.py` ranks a collection of models by calculating the CD (referred within the code as `validation_loss`) of each of these models  w.r.t. the evaluation set. The CD of a model is based on the distance metrics across all rhythmic features; as such, this module requires the output of the evaluation model.

### Arguments

* **Evaluation run results paths** A list of the paths of the evaluation results for the to-be-ranked models
* **Traning Report path** A `.csv` file that details (at least) the data augmentation parameter configurations for each of the to-be-ranked models (identified by model training start time). If the models' training was tracked with wandb, then this csv can be downloaded from there.
* **Number of models** If `num_models = -1`, all models will be considered for the analysis. Otherwise, only the top n models will be considered.
* **Reduced features** Bool that indicates whether to use the complete-feature-set or the reduced-feature-set to calculate CD.
* **baseline_path** Path for a baseline model (aka a non-augmented model). If provided, the baseline model is placed at the top of all the other models regardless of its CD for comparison purposes.

### Output

A `.csv` file where each row corresponds to a model. Models are ordered from top to bottom, with the topmost having the smallest CD and the bottommost the largest. The data augmentation parameters of each model are showcased in this csv.

### Detailed analysis

The script `detailed_analysis.py` performs multiple operations:

1. Run both reduced-feature-set-analysis and complete-feature-set-analysis for a collection of models. 
2. Randomly selects `num_audio_samples` examples from the evaluation set and synthesizes the corresponding generated examples across all models.
3. Creates a scatterplot for each model that visualizes distance metrics.

#### Usage

1. Move models to be analyzed to the `models/` dir. One of these models has to be a baseline model, else the script will crash.
2. Move corresponding evaluation results to the `eval_runs/` dir.
3. Move the training report to the root dir. Change the `REPORT_PATH` to the name of the report.
4. ``` $ python detailed_analysis.py$ ```

Output will be written to `analysis_out`.

